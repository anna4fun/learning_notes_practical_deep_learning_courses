{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook is a fork of the FastAI class's stable diffusion deep dive.\n",
    "\n",
    "# Setup and Import the Stable Diffusion models"
   ],
   "id": "4265d5b5d9b2f033"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:27:40.633020Z",
     "start_time": "2025-10-07T22:27:40.629724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ],
   "id": "48ae580995388088",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/usr/bin/python3\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:25:08.461968Z",
     "start_time": "2025-10-07T22:25:07.348970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import accelerate\n",
    "    print(f\"accelerate is installed: version {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"accelerate is NOT installed in this environment\")"
   ],
   "id": "69205d5c3801c192",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lcjh/Library/Python/3.9/lib/python/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate is installed: version 1.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lcjh/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-07T22:27:26.838612Z",
     "start_time": "2025-10-07T22:27:23.217246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# For video display:\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "import os\n",
    "\n",
    "torch.manual_seed(100)\n",
    "if not (Path.home()/'.cache/huggingface'/'token').exists(): notebook_login()\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "logging.set_verbosity_error()\n"
   ],
   "id": "3b061ff90e67dd7f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:27:49.652617Z",
     "start_time": "2025-10-07T22:27:49.624800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ],
   "id": "c22abcc92a34e78",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load the models from HuggingFace\n",
    "See what's in the huggingface downloaded folder"
   ],
   "id": "d7974c99e18725d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:27:52.305170Z",
     "start_time": "2025-10-07T22:27:52.171410Z"
    }
   },
   "cell_type": "code",
   "source": "!ls ~/.cache/huggingface",
   "id": "fe1c57d528a757eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34mhub\u001B[m\u001B[m           stored_tokens token         \u001B[34mxet\u001B[m\u001B[m\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:27:53.818194Z",
     "start_time": "2025-10-07T22:27:53.686570Z"
    }
   },
   "cell_type": "code",
   "source": "!ls ~/.cache/huggingface/xet",
   "id": "5d5c4c9637be3f52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34mhttps___cas_serv-tGqkUaZf_CBPHQ6h\u001B[m\u001B[m\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:27:55.314854Z",
     "start_time": "2025-10-07T22:27:55.183776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this is the model I have downloaded from huggingface in other notebook\n",
    "!ls ~/.cache/huggingface/hub"
   ],
   "id": "6c3d0d2f2d68581b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[34mmodels--CompVis--stable-diffusion-v1-4\u001B[m\u001B[m \u001B[34mmodels--openai--clip-vit-large-patch14\u001B[m\u001B[m\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**VAE**: variational auto-encoder. this module is responsible for encode images into latents and then decode the latents back to the image. Stable diffusion is only gonna use the latents generated by the encoding step. Why we need the latents? Memory saving! Latents are compressed version of the original image with much smaller size in terms of bytes/pixels. The latents will be services as the input image of the UNet.\n",
    "\n",
    "Noted that the encoder and decoder of VAE are both neural nets, with the encoder as the convolutional layers and decoder as transpose convolutional layers. So don't forget to move VAE to GPU.\n",
    "\n",
    "KL: KL divergence is used to compute the similarities between the (normal) distribusions of the noise between the encode and decode process. It helps us force the encode and decode process are indeed the same."
   ],
   "id": "1aaeffc2763379f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:28:05.713108Z",
     "start_time": "2025-10-07T22:28:05.002765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vae\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)"
   ],
   "id": "cb36e557f7398f11",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:28:07.051234Z",
     "start_time": "2025-10-07T22:28:07.045633Z"
    }
   },
   "cell_type": "code",
   "source": "type(vae)",
   "id": "87e67f30d4dcb673",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**CLIP**: contrastive loss image pretrain, this is the process that trains the embedding that maps the image(eg. a cat picture) and its caption(eg. \"cat\") into adjacent embedding vector space. ~~Note that CLIP train 2 tokenizers: image-to-embedding and text-to-embedding image encoder and text encoder. For stable diffusion, we are only gonna use the text-to-embedding part,~~\n",
    "Note that CLIP trains 2 encoders (image-encoder and text-encoder), through contrastive learning, CLIP align the outputs of the 2 encoders to produce one unified embedding space, enabling cross-modal similarity.\n",
    "\n",
    "We are only gonna use the text-encoder and it will serve as frozen-language-feature-extractor(aka, the machine that convert the caption/prompt into vector) as input of the UNet, both for building the guidance vector during training and building prompt vector during inference.\n",
    "\n",
    "Are the embeddings the same as CLIP’s original ones?\n",
    "Not exactly — and this is an important nuance.\n",
    "\n",
    "- If you use the exact same CLIP checkpoint (say, OpenAI’s ViT-L/14), then encoding \"dog\" with that model yields the same vector that CLIP would produce for \"dog\".\n",
    "\n",
    "- But Stable Diffusion may use a different text encoder checkpoint (e.g., OpenCLIP ViT-H/14 trained on LAION). The architecture is CLIP-like, but weights differ. Therefore, the vector for \"dog\" will not be numerically identical, though semantically similar.\n",
    "\n",
    "Also, SD sometimes fine-tunes or freezes only part of the encoder — depending on the model version.\n",
    "\n",
    "🧩 Conceptually the same type of embedding, but not literally the same numbers as the original CLIP model trained jointly with an image encoder.\n",
    "\n",
    "Note that UNet is not gonna use the image-encoder and the unified embedding produced by CLIP.\n",
    "\n",
    "Tips:\n",
    "1. Rule of thumb for the `to(device)`: only the Pytorch neural net models need to be move to GPU(mps) for acceleration, things like tokenizer, schedulers are not neural nets that contains millions of parameters so they are considered \"lightweight\" and can stay on CPU.\n",
    "2. I forgot the tokenizer part in the very begining: captions like \"a brown cat on the street\" should first be tokenized and then feed into text-encoders such as CLIP."
   ],
   "id": "e870e6a5fbd57b4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:28:13.923308Z",
     "start_time": "2025-10-07T22:28:12.896690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)"
   ],
   "id": "458b06068a7d5fd5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:28:18.753982Z",
     "start_time": "2025-10-07T22:28:18.748405Z"
    }
   },
   "cell_type": "code",
   "source": "type(tokenizer), type(text_encoder)",
   "id": "31d24aeaa5ba4bc8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.clip.tokenization_clip.CLIPTokenizer,\n",
       " transformers.models.clip.modeling_clip.CLIPTextModel)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**U-Net**: the diffusion model that intakes image(latents) and caption(text embeddin), gradually add noise to the image in a series of steps until it turns into a complete noise (forward process), and then by reversing the forward process to reduce noise from the noise in the same series of steps until the noise turns back to the image (not the original image but an averaged image). The word \"U\" means this forward and reverse procces looks like a U-shaped funnel.",
   "id": "fe9f0e729b940cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:28:26.205653Z",
     "start_time": "2025-10-07T22:28:22.114196Z"
    }
   },
   "cell_type": "code",
   "source": "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\").to(device)",
   "id": "2ecb9d4c22b72ffc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "@TODO: what is a scheduler? is this the one that manage how much noise to add to the image in the Unet training?",
   "id": "9a386b82d480991f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:30:59.563533Z",
     "start_time": "2025-10-07T22:30:59.526225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012,\n",
    "beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
   ],
   "id": "d6ce4ca1255b548c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f1e49ec8e01fa854"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# A diffusion loop\n",
    "\n",
    "@TODO:\n",
    "1. remind me of what is a classier-free guidance? is it just a guidance/caption of what the image is about? (instead of a tyical classifier label?)\n",
    "2. why are we starting with one prompt for inference? I thought we are going to inspect the training of a Unet"
   ],
   "id": "7baaa33be388fc9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:53:47.995029Z",
     "start_time": "2025-10-07T22:53:47.992633Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "b8d6bced76f9ac50",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:53:51.582725Z",
     "start_time": "2025-10-07T22:53:51.577327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# some settings\n",
    "prompt = [\"a white and grey rag doll cat\"]\n",
    "height = 512\n",
    "width = 512\n",
    "num_inference_steps = 30 # denoising steps\n",
    "guidance_scale = 7.5 # scale for classifier-free guidance\n",
    "generator = torch.manual_seed(32) # seed generator to create the initial latent noise\n",
    "batch_size = 1 #does batch_size = 1 means we only want 1 image?"
   ],
   "id": "c2e0b8531db03c2c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## tokenize the prompt and then use the CLIP text encoder to turn it into embeddings\n",
    "\n",
    "### First let's generate the embedding of the actual prompt (conditional)"
   ],
   "id": "dde8ff078c745dc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:57:20.399265Z",
     "start_time": "2025-10-07T22:57:20.389756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "#tensors in plural"
   ],
   "id": "93959bccc9d840c",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:51:13.386328Z",
     "start_time": "2025-10-07T22:51:13.381471Z"
    }
   },
   "cell_type": "code",
   "source": "text_input.input_ids",
   "id": "fa62a87cf5f2c1a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49406,\n",
       " 320,\n",
       " 1579,\n",
       " 537,\n",
       " 5046,\n",
       " 20687,\n",
       " 9286,\n",
       " 2368,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407,\n",
       " 49407]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "what is input_id and attention_mask?\n",
    "\n",
    "why is input_id contains so many 49407? I thought tokenization is like dictionary checking agains the vocabulary in the tokenizer model"
   ],
   "id": "ab5ff5afd10b1856"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T23:03:29.510720Z",
     "start_time": "2025-10-07T23:03:29.505159Z"
    }
   },
   "cell_type": "code",
   "source": "max_length = text_input.input_ids.shape[-1]",
   "id": "ef50b224ea641aac",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "❓ Why torch.no_grad here? have we initialized some gradients in the text_input and we are gonna updates the coefficients?\n",
    "\n",
    "No, it's an optimization for speed and memory, not about preventing unintended training.\n",
    "Rule of thumb: for inference, always do `torch.no_grad()`, with gradient, text encode eat up 1.5GB VRAM, without gradients ~500MB VRAM."
   ],
   "id": "1c1939e384efd09f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:57:58.633768Z",
     "start_time": "2025-10-07T22:57:57.475930Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 35,
   "source": [
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]"
   ],
   "id": "cdc74706f28f73b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T23:02:19.431750Z",
     "start_time": "2025-10-07T23:02:19.429019Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37,
   "source": "type(text_embeddings)",
   "id": "d30f05c6eb89cfc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T23:02:28.442236Z",
     "start_time": "2025-10-07T23:02:28.412555Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38,
   "source": "text_embeddings.ndim",
   "id": "3393b8aa8bf26dfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "88629dd2ecdd3dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Then generate an empty prompt embedding (the unconditional). A key to classifier-free guidance",
   "id": "b80f750c8362342f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T23:04:02.039427Z",
     "start_time": "2025-10-07T23:04:02.034252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uncond_input = tokenizer(\n",
    "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")"
   ],
   "id": "28acd11e270a5077",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T23:04:15.568309Z",
     "start_time": "2025-10-07T23:04:15.562015Z"
    }
   },
   "cell_type": "code",
   "source": "uncond_input",
   "id": "3f34dedf9a9e4de0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T23:07:55.576992Z",
     "start_time": "2025-10-07T23:07:55.529962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(device))[0]"
   ],
   "id": "447d9f27e068f38",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Concat the conditional and unconditional embeddings into 1 tensor",
   "id": "5a2388a8f20f8c8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T23:08:19.786538Z",
     "start_time": "2025-10-07T23:08:19.774904Z"
    }
   },
   "cell_type": "code",
   "source": "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])",
   "id": "11e752d7d6016c0b",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare Scheduler and Latents",
   "id": "f640cc1099458e88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b811df08153a80c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d37634d4f767058b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
