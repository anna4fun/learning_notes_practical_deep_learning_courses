{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-10T18:43:48.215616Z",
     "start_time": "2025-10-10T18:43:41.023981Z"
    }
   },
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lcjh/Library/Python/3.9/lib/python/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/Users/lcjh/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:44:13.516283Z",
     "start_time": "2025-10-10T18:44:13.512733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import accelerate\n",
    "    print(f\"accelerate is installed: version {accelerate.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"accelerate is NOT installed in this environment\")"
   ],
   "id": "688597d01dd21b5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate is installed: version 1.10.1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:44:29.877580Z",
     "start_time": "2025-10-10T18:44:29.855658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ],
   "id": "ddb01e10cf369a92",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:46:16.744330Z",
     "start_time": "2025-10-10T18:46:16.479735Z"
    }
   },
   "cell_type": "code",
   "source": "!ls",
   "id": "5e8a25ee1eaaa195",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP_Embedding_samples_cats_pineapples.ipynb\r\n",
      "CLIP_intro.png\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T18:45:17.235765Z",
     "start_time": "2025-10-10T18:44:56.272932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Load pretrained CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "id": "e5d466d773929c99",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "797cba439b0444a2bbd8e1d03dc4c07c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cba0accbd4004bdd869fadef51920705"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ef6ff2bcb8540669fe26497794f2605"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65faed3df31e46a6a2fa8ccf660de124"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c286b89dadad4494a467c99ae29421da"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9398336184d445c5be2d52de5a104507"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dcceb0e3b8834705bbb53c420a425464"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9b5508aedc54e2aa6ca3138f27fbb74"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22908e4c78ea47aa99d9f1faa14bc100"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/lcjh/Documents/GitHub/learning_notes_practical_deep_learning_courses/CLIP_Contrastive_Language_Image_Pretrain/cat.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m processor \u001B[38;5;241m=\u001B[39m CLIPProcessor\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mopenai/clip-vit-base-patch32\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# 2. Load example images\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m cat_image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcat.jpg\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m         \u001B[38;5;66;03m# <-- replace with your own file path\u001B[39;00m\n\u001B[1;32m      7\u001B[0m pineapple_image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpineapple.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/PIL/Image.py:3469\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3466\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[1;32m   3468\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[0;32m-> 3469\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3470\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   3471\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/lcjh/Documents/GitHub/learning_notes_practical_deep_learning_courses/CLIP_Contrastive_Language_Image_Pretrain/cat.jpg'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T19:55:18.008386Z",
     "start_time": "2025-10-10T19:55:17.998196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Load example images\n",
    "cat_image = Image.open(\"images/cat.png\")\n",
    "pineapple_image = Image.open(\"images/pineapple.png\")"
   ],
   "id": "974e2a74cb66c7b9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T19:55:19.304063Z",
     "start_time": "2025-10-10T19:55:19.262417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Define the text prompts\n",
    "texts = [\"a photo of a cat\", \"a photo of a pineapple\"]\n",
    "\n",
    "# 4. Preprocess the data\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=[cat_image, pineapple_image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")"
   ],
   "id": "d3c7f0745af88fe1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T19:55:21.389722Z",
     "start_time": "2025-10-10T19:55:21.204093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    image_embeds = outputs.image_embeds    # shape [2, 512]\n",
    "    text_embeds = outputs.text_embeds      # shape [2, 512]"
   ],
   "id": "7c220045f5f7d49d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T19:55:22.747530Z",
     "start_time": "2025-10-10T19:55:22.741463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. Normalize embeddings (L2 normalization)\n",
    "image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)"
   ],
   "id": "21ec44244cd20b7a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T19:55:24.325004Z",
     "start_time": "2025-10-10T19:55:24.317225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. Compute cosine similarities\n",
    "cosine_sim = image_embeds @ text_embeds.T  # 2x2 matrix\n",
    "print(\"Cosine similarity matrix:\")\n",
    "print(cosine_sim)"
   ],
   "id": "cbe62fd070752489",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix:\n",
      "tensor([[0.2832, 0.2036],\n",
      "        [0.1956, 0.3286]])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "398b59543b2cabcd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
